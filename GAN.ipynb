{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ2yGt2CBHG1"
   },
   "source": [
    "We investigate semi supervisede learning techniques on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1702,
     "status": "ok",
     "timestamp": 1616397113149,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "-FP7H8zgtuYQ"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBw2Mp8JtuYW"
   },
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2126,
     "status": "ok",
     "timestamp": 1616397113581,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "LSfvChhstuYX",
    "outputId": "9feaf929-3ca0-4621-d5c6-14c92d64c337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load MNIST data\n",
    "(train_x, train_y),(test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "#Shape\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2121,
     "status": "ok",
     "timestamp": 1616397113583,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "5tgKko1otuYd"
   },
   "outputs": [],
   "source": [
    "#Reshape images to be 3D\n",
    "train_x = np.reshape(train_x, (-1,28,28,1))\n",
    "test_x = np.reshape(test_x, (-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2116,
     "status": "ok",
     "timestamp": 1616397113584,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "2TMfEZv7tuYg",
    "outputId": "f280c854-747e-40ec-cc46-857971f0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2112,
     "status": "ok",
     "timestamp": 1616397113586,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "_bwO_LM0tuYk"
   },
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "train_x = train_x/127.5 - 1\n",
    "test_x = test_x/127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9zlzXEituYo"
   },
   "source": [
    "Split Training Data between Supervised and Unsupervised Examples. 10% of the data will be used in Supervised learning while rest of it will be used for UnSupervised Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2109,
     "status": "ok",
     "timestamp": 1616397113587,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "qU9mN_9btuYp"
   },
   "outputs": [],
   "source": [
    "supervised_data_percent = 0.015\n",
    "unsupervised_data_percent = 1 - supervised_data_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2106,
     "status": "ok",
     "timestamp": 1616397113589,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "JtaNoqr4tuYs"
   },
   "outputs": [],
   "source": [
    "train_x_sup, train_x_unsup, train_y_sup, train_y_unsup = train_test_split(train_x, train_y, \n",
    "                                                                          train_size=supervised_data_percent,\n",
    "                                                                          test_size=unsupervised_data_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2103,
     "status": "ok",
     "timestamp": 1616397113590,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "axIoHDsftuYv",
    "outputId": "e65856f2-8b5d-4b53-e7b8-11b55087f730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 28, 28, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_sup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFBiWddYtuYy"
   },
   "source": [
    "Following function will do 2 things:\n",
    "\n",
    "1. Convert MNIST labels to One-hot encoding\n",
    "2. Append a column at the end with zeros to indicate Real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1616397113592,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "g64eSPJRtuYz"
   },
   "outputs": [],
   "source": [
    "def prepare_labels(y):\n",
    "    \n",
    "    extended_labels = tf.keras.utils.to_categorical(y, 10)\n",
    "    extended_labels = np.concatenate([extended_labels, np.zeros((extended_labels.shape[0],1))], axis=1)\n",
    "    \n",
    "    return extended_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIfZbflZtuY1"
   },
   "source": [
    "### Build Generator\n",
    "\n",
    "Generator will take 100 random numbers as input and will produce an image of shape (28,28,1). Image data values will be between -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2096,
     "status": "ok",
     "timestamp": 1616397113593,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "Ms66ffRMtuY2"
   },
   "outputs": [],
   "source": [
    "def generator(input_x, training, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "        \n",
    "        #Layer 0\n",
    "        x = tf.keras.layers.Reshape((1,1,100,))(input_x)\n",
    "        \n",
    "        #Layer 1\n",
    "        x = tf.keras.layers.Conv2DTranspose(100, kernel_size=(2,2), strides=1, padding='valid')(x)\n",
    "        x = tf.layers.batch_normalization(x, training=training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        \n",
    "        #Layer 2\n",
    "        x = tf.keras.layers.Conv2DTranspose(64, kernel_size=(3,3), strides=2, padding='valid')(x)\n",
    "        x = tf.layers.batch_normalization(x, training=training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        \n",
    "        #Layer 3\n",
    "        x = tf.keras.layers.Conv2DTranspose(32, kernel_size=(4,4), strides=2, padding='valid')(x)\n",
    "        x = tf.layers.batch_normalization(x, training=training)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        \n",
    "        #Layer 4\n",
    "        x = tf.keras.layers.Conv2DTranspose(1, kernel_size=(6,6), strides=2, padding='valid')(x)\n",
    "        x = tf.keras.activations.tanh(x)\n",
    "        \n",
    "        return x       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdMOnym2tuY4"
   },
   "source": [
    "### Build Discriminator\n",
    "\n",
    "Discriminator will Images of shape (28,28,1) as input and will produce a vector with 11 values.\n",
    "\n",
    "- 10 Values for MNIST label Classification\n",
    "- 1 Value for Classifying if image is Fake(1) OR Real(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1616397113594,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "B6r_W6QKtuY5"
   },
   "outputs": [],
   "source": [
    "def discriminator(input_d, p_drop, reuse=True, training = True):\n",
    "    \n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "        \n",
    "        #Layer 1\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(5,5), strides=2, padding='same')(input_d)\n",
    "        x = tf.keras.layers.Dropout(p_drop)(x)\n",
    "        x = tf.keras.activations.relu(x, alpha=0.2)\n",
    "        \n",
    "        #Layer 2\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = tf.layers.batch_normalization(x, training=training)\n",
    "        x = tf.keras.activations.relu(x, alpha=0.2)\n",
    "        \n",
    "        #Layer 3\n",
    "        x = tf.keras.layers.Conv2D(128, kernel_size=(2,2), strides=2, padding='same')(x)\n",
    "        x = tf.layers.batch_normalization(x, training=training)\n",
    "        x = tf.keras.activations.relu(x, alpha=0.2)\n",
    "        x = tf.keras.layers.Dropout(p_drop)(x)\n",
    "        \n",
    "        #Layer 4\n",
    "        x = tf.keras.layers.Conv2D(128, kernel_size=(2,2), strides=2, padding='same')(x)\n",
    "        x = tf.keras.activations.relu(x, alpha=0.2)\n",
    "        \n",
    "        #Layer 5\n",
    "        features = tf.keras.layers.Flatten()(x)\n",
    "        logits = tf.keras.layers.Dense(11)(features)\n",
    "        \n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyfWHDq-tuY7"
   },
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss will be calculated for the following 3 inputs:\n",
    "\n",
    "1. Real images with actual labels (Supervised Learning)\n",
    "2. Real images with NO labels (Unsupervised Learning)\n",
    "3. Fake images with NO labels (Unsupervised Learning)\n",
    "\n",
    "\n",
    "Loss will be calculated for Discriminator and Generator. \n",
    "\n",
    "#### 1. Discriminator Loss\n",
    "\n",
    "Following will be considered to calculate Loss:\n",
    "\n",
    "Unsupervised:\n",
    "1. Loss to predict Real Image is Real and Not fake.\n",
    "2. Loss to predict Fake Image is Fake and Not Real.\n",
    "\n",
    "Supervised:\n",
    "1. Loss to predict MNIST label classification\n",
    "\n",
    "#### 2. Generator Loss\n",
    "\n",
    "Unsupervised Loss:\n",
    "1. Loss to predict Fake Image as Real\n",
    "2. Feature Mapping loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1616397113595,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "aGHXmCoUtuY8"
   },
   "outputs": [],
   "source": [
    "def model_loss(real_un_sup_ip, real_sup_ip, fake_ip, p_drop, training, y):\n",
    "    \n",
    "        \n",
    "    #Get Discriminator output for Real Supervised Data\n",
    "    rs_features, rs_logits = discriminator(real_sup_ip, p_drop, reuse=False, training=training)\n",
    "    \n",
    "    #Get Discriminator output for Real Un-Supervised Data\n",
    "    ru_features, ru_logits = discriminator(real_un_sup_ip, p_drop, reuse=True, training=training)\n",
    "    \n",
    "    #Get Fake images from Generator\n",
    "    fake_images = generator(fake_ip, training=training)\n",
    "    \n",
    "    #Get Dicriminator output for Fake images\n",
    "    fake_features, fake_logits = discriminator(fake_images, p_drop, reuse=True, training=training)\n",
    "    \n",
    "    \n",
    "    #Calculating Discriminator Loss\n",
    "    \n",
    "    #1. Let's calculate Unsupervised Loss for both Real and Fake data\n",
    "    real_un_sup_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=ru_logits[:,-1], \n",
    "                                                                              labels=tf.zeros_like(ru_logits[:,-1])))\n",
    "        \n",
    "    \n",
    "    fake_un_sup_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits[:,-1], \n",
    "                                                                              labels=tf.ones_like(fake_logits[:,-1])))\n",
    "    \n",
    "    #2. Supervised Loss\n",
    "    real_sup_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=rs_logits, \n",
    "                                                                              labels=y))\n",
    "    \n",
    "    d_loss = real_un_sup_loss + fake_un_sup_loss + real_sup_loss\n",
    "    \n",
    "    \n",
    "    #Calculating feature mapping loss for Generator\n",
    "    tmp1 = tf.reduce_mean(ru_features, axis = 0)\n",
    "    tmp2 = tf.reduce_mean(fake_features, axis = 0)\n",
    "    feature_mapping_loss = tf.reduce_mean(tf.square(tmp1 - tmp2))\n",
    "    \n",
    "    #Fake vs Real loss\n",
    "    fake_loss_2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits[:,-1], \n",
    "                                                                              labels=tf.zeros_like(fake_logits[:,-1])))\n",
    "    \n",
    "    #g_loss = feature_mapping_loss +  fake_loss_2\n",
    "    g_loss = fake_loss_2\n",
    "    \n",
    "    rs_class_op = tf.nn.softmax(rs_logits)\n",
    "    \n",
    "    #Calculate Accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(rs_class_op, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return fake_images, d_loss, g_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "373EM-j9tuZA"
   },
   "source": [
    "### Model Optimization\n",
    "\n",
    "Training Discriminator and Generator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2089,
     "status": "ok",
     "timestamp": 1616397113597,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "8lDFl_5ttuZB"
   },
   "outputs": [],
   "source": [
    "def model_optimization(d_loss, g_loss):\n",
    "    \n",
    "    # Get weights and biases to update. Get them separately for the discriminator and the generator\n",
    "    discriminator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES , scope='Discriminator')    \n",
    "    generator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "    \n",
    "    #Minimize loss\n",
    "    d_opt = tf.train.AdamOptimizer(name='d_optimizer').minimize(d_loss, var_list=discriminator_train_vars)\n",
    "    \n",
    "    g_opt = tf.train.AdamOptimizer(name='g_optimizer').minimize(g_loss, var_list=generator_train_vars)\n",
    "    \n",
    "    return d_opt, g_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrRwdH02tuZE"
   },
   "source": [
    "### Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1616398160489,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "8EOS77H4tuZF"
   },
   "outputs": [],
   "source": [
    "def train(batch_size = 64, epochs = 1000):\n",
    "    \n",
    "    train_D_losses = []\n",
    "    train_G_losses = []\n",
    "    train_Accs  = []\n",
    "    test_D_losses = []\n",
    "    test_G_losses = []\n",
    "    test_Accs = []\n",
    "    noise_size = 100\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    #Declare Placeholders for input values\n",
    "    real_sup_img = tf.placeholder(tf.float32, shape=(None,28,28,1))\n",
    "    labels = tf.placeholder(tf.int64, shape=(None))\n",
    "    \n",
    "    real_unsup_img = tf.placeholder(tf.float32, shape=(None,28,28,1))\n",
    "    \n",
    "    noise_input = tf.placeholder(tf.float32, shape=(None, noise_size))\n",
    "    \n",
    "    dropout_rate = tf.placeholder(tf.float32)\n",
    "    training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #Learning rate for Generator and Discriminator\n",
    "    lr_g = tf.placeholder(tf.float32)\n",
    "    lr_d = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    #Build the Graph\n",
    "    fake_images, d_loss, g_loss, accuracy = model_loss(real_unsup_img, real_sup_img, noise_input, dropout_rate, \n",
    "                                                       training, labels)    \n",
    "    d_opt, g_opt = model_optimization(d_loss, g_loss)\n",
    "    \n",
    "    \n",
    "    #Execute Graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            #90% real images will be unsupervised\n",
    "            unsup_indexes = np.random.randint(0, train_x_unsup.shape[0], size=int(0.9*batch_size))\n",
    "            #10% of images will be supervised\n",
    "            sup_indexes = np.random.randint(0, train_x_sup.shape[0], size=int(0.1*batch_size))\n",
    "            \n",
    "            \n",
    "            train_feed_dict = {real_sup_img: train_x_sup[sup_indexes], \n",
    "                         labels: prepare_labels(train_y_sup[sup_indexes]), \n",
    "                         real_unsup_img: train_x_unsup[unsup_indexes], \n",
    "                         noise_input: np.random.uniform(-1.0, 1.0, size = (batch_size, 100)), \n",
    "                         dropout_rate: 0.5,\n",
    "                         training: True,\n",
    "                         lr_g: 1e-5, \n",
    "                         lr_d: 1e-5}\n",
    "            \n",
    "            _,_, dloss, gloss, acc = sess.run([d_opt, g_opt, d_loss, g_loss, accuracy], feed_dict=train_feed_dict)\n",
    "            \n",
    "            \n",
    "            #Calculate Loss and Accuracy for Test Data\n",
    "            if i % 200 == 0:\n",
    "                \n",
    "                print(i, '. Training Acc', acc, end='\\t')\n",
    "                train_Accs.append(acc)\n",
    "                \n",
    "                test_feed_dict = {real_sup_img: test_x, \n",
    "                         labels: prepare_labels(test_y), \n",
    "                         real_unsup_img: test_x, \n",
    "                         noise_input: np.random.uniform(-1.0, 1.0, size = (batch_size, 100)), \n",
    "                         dropout_rate: 0,\n",
    "                         training: False}\n",
    "                \n",
    "                t_dloss, t_gloss, t_acc, fakeImgs = sess.run([d_loss, g_loss, accuracy, fake_images], \n",
    "                                                             feed_dict=test_feed_dict)\n",
    "                \n",
    "                test_Accs.append(t_acc)\n",
    "                \n",
    "                print('Test Acc', t_acc)\n",
    "    return train_Accs, test_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544850,
     "status": "ok",
     "timestamp": 1616398707478,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "dtgFn-fvtuZI",
    "outputId": "4c25c055-b4c0-4c12-e3b2-10dab0012177"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  '`tf.layers.batch_normalization` is deprecated and '\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 . Training Acc 0.0\tTest Acc 0.159\n",
      "200 . Training Acc 1.0\tTest Acc 0.6312\n",
      "400 . Training Acc 0.6666667\tTest Acc 0.6751\n",
      "600 . Training Acc 1.0\tTest Acc 0.6751\n",
      "800 . Training Acc 1.0\tTest Acc 0.8403\n",
      "1000 . Training Acc 1.0\tTest Acc 0.8104\n",
      "1200 . Training Acc 1.0\tTest Acc 0.838\n",
      "1400 . Training Acc 1.0\tTest Acc 0.7919\n",
      "1600 . Training Acc 1.0\tTest Acc 0.8792\n",
      "1800 . Training Acc 1.0\tTest Acc 0.8446\n",
      "2000 . Training Acc 1.0\tTest Acc 0.8888\n",
      "2200 . Training Acc 1.0\tTest Acc 0.8651\n",
      "2400 . Training Acc 1.0\tTest Acc 0.8885\n",
      "2600 . Training Acc 1.0\tTest Acc 0.8835\n",
      "2800 . Training Acc 1.0\tTest Acc 0.9048\n",
      "3000 . Training Acc 1.0\tTest Acc 0.8366\n",
      "3200 . Training Acc 1.0\tTest Acc 0.8142\n",
      "3400 . Training Acc 1.0\tTest Acc 0.9\n",
      "3600 . Training Acc 1.0\tTest Acc 0.8822\n",
      "3800 . Training Acc 1.0\tTest Acc 0.8608\n",
      "4000 . Training Acc 1.0\tTest Acc 0.7773\n",
      "4200 . Training Acc 1.0\tTest Acc 0.8044\n",
      "4400 . Training Acc 1.0\tTest Acc 0.8992\n",
      "4600 . Training Acc 1.0\tTest Acc 0.9045\n",
      "4800 . Training Acc 1.0\tTest Acc 0.8886\n",
      "5000 . Training Acc 1.0\tTest Acc 0.8792\n",
      "5200 . Training Acc 1.0\tTest Acc 0.8982\n",
      "5400 . Training Acc 1.0\tTest Acc 0.9151\n",
      "5600 . Training Acc 1.0\tTest Acc 0.9184\n",
      "5800 . Training Acc 1.0\tTest Acc 0.9162\n",
      "6000 . Training Acc 0.6666667\tTest Acc 0.8653\n",
      "6200 . Training Acc 1.0\tTest Acc 0.8612\n",
      "6400 . Training Acc 1.0\tTest Acc 0.8647\n",
      "6600 . Training Acc 0.6666667\tTest Acc 0.8341\n",
      "6800 . Training Acc 1.0\tTest Acc 0.8304\n",
      "7000 . Training Acc 1.0\tTest Acc 0.8379\n",
      "7200 . Training Acc 1.0\tTest Acc 0.906\n",
      "7400 . Training Acc 1.0\tTest Acc 0.8802\n",
      "7600 . Training Acc 1.0\tTest Acc 0.9067\n",
      "7800 . Training Acc 1.0\tTest Acc 0.904\n",
      "8000 . Training Acc 1.0\tTest Acc 0.9071\n",
      "8200 . Training Acc 1.0\tTest Acc 0.9045\n",
      "8400 . Training Acc 1.0\tTest Acc 0.9056\n",
      "8600 . Training Acc 1.0\tTest Acc 0.9095\n",
      "8800 . Training Acc 1.0\tTest Acc 0.9085\n",
      "9000 . Training Acc 1.0\tTest Acc 0.9041\n",
      "9200 . Training Acc 1.0\tTest Acc 0.909\n",
      "9400 . Training Acc 1.0\tTest Acc 0.9102\n",
      "9600 . Training Acc 1.0\tTest Acc 0.9109\n",
      "9800 . Training Acc 1.0\tTest Acc 0.9099\n",
      "10000 . Training Acc 1.0\tTest Acc 0.9091\n",
      "10200 . Training Acc 1.0\tTest Acc 0.9109\n",
      "10400 . Training Acc 1.0\tTest Acc 0.9121\n",
      "10600 . Training Acc 1.0\tTest Acc 0.9134\n",
      "10800 . Training Acc 1.0\tTest Acc 0.9136\n",
      "11000 . Training Acc 1.0\tTest Acc 0.9157\n",
      "11200 . Training Acc 1.0\tTest Acc 0.9155\n",
      "11400 . Training Acc 1.0\tTest Acc 0.9158\n",
      "11600 . Training Acc 1.0\tTest Acc 0.9171\n",
      "11800 . Training Acc 1.0\tTest Acc 0.9184\n",
      "12000 . Training Acc 1.0\tTest Acc 0.9214\n",
      "12200 . Training Acc 1.0\tTest Acc 0.9224\n",
      "12400 . Training Acc 1.0\tTest Acc 0.9207\n",
      "12600 . Training Acc 1.0\tTest Acc 0.9201\n",
      "12800 . Training Acc 1.0\tTest Acc 0.9206\n",
      "13000 . Training Acc 1.0\tTest Acc 0.9209\n",
      "13200 . Training Acc 1.0\tTest Acc 0.9221\n",
      "13400 . Training Acc 1.0\tTest Acc 0.9214\n",
      "13600 . Training Acc 1.0\tTest Acc 0.9203\n",
      "13800 . Training Acc 1.0\tTest Acc 0.9193\n",
      "14000 . Training Acc 1.0\tTest Acc 0.9201\n",
      "14200 . Training Acc 1.0\tTest Acc 0.9214\n",
      "14400 . Training Acc 1.0\tTest Acc 0.9225\n",
      "14600 . Training Acc 1.0\tTest Acc 0.922\n",
      "14800 . Training Acc 1.0\tTest Acc 0.9226\n",
      "15000 . Training Acc 1.0\tTest Acc 0.9224\n",
      "15200 . Training Acc 1.0\tTest Acc 0.922\n",
      "15400 . Training Acc 1.0\tTest Acc 0.9219\n",
      "15600 . Training Acc 1.0\tTest Acc 0.9239\n",
      "15800 . Training Acc 1.0\tTest Acc 0.9246\n",
      "16000 . Training Acc 1.0\tTest Acc 0.9243\n",
      "16200 . Training Acc 1.0\tTest Acc 0.9236\n",
      "16400 . Training Acc 1.0\tTest Acc 0.925\n",
      "16600 . Training Acc 1.0\tTest Acc 0.924\n",
      "16800 . Training Acc 1.0\tTest Acc 0.924\n",
      "17000 . Training Acc 1.0\tTest Acc 0.9246\n",
      "17200 . Training Acc 1.0\tTest Acc 0.9277\n",
      "17400 . Training Acc 1.0\tTest Acc 0.9253\n",
      "17600 . Training Acc 1.0\tTest Acc 0.9256\n",
      "17800 . Training Acc 1.0\tTest Acc 0.9243\n",
      "18000 . Training Acc 1.0\tTest Acc 0.9252\n",
      "18200 . Training Acc 1.0\tTest Acc 0.9259\n",
      "18400 . Training Acc 1.0\tTest Acc 0.9268\n",
      "18600 . Training Acc 1.0\tTest Acc 0.9242\n",
      "18800 . Training Acc 1.0\tTest Acc 0.9243\n",
      "19000 . Training Acc 1.0\tTest Acc 0.9233\n",
      "19200 . Training Acc 1.0\tTest Acc 0.9238\n",
      "19400 . Training Acc 1.0\tTest Acc 0.9259\n",
      "19600 . Training Acc 1.0\tTest Acc 0.9257\n",
      "19800 . Training Acc 1.0\tTest Acc 0.9257\n"
     ]
    }
   ],
   "source": [
    "accs, val_accs = train(batch_size=32,epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2068,
     "status": "aborted",
     "timestamp": 1616397113598,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "e7FmDa5TtuZM"
   },
   "outputs": [],
   "source": [
    "def plot_images(fake_images):\n",
    "    \n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = 16\n",
    "    \n",
    "    image_size = 28\n",
    "    rows = 4\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(fake_images[i], [image_size, image_size])\n",
    "        image = (image + 1)/2\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2065,
     "status": "aborted",
     "timestamp": 1616397113599,
     "user": {
      "displayName": "Sutithi Chakraborty",
      "photoUrl": "",
      "userId": "12749148906436566424"
     },
     "user_tz": -330
    },
    "id": "KRMuzkP1Bjle"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3. Semi-Supervised Learning GANs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
